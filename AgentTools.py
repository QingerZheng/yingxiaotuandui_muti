import json
from typing import List, Any

from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from states import AgentState, DebugInfo, EmotionalState
from langchain_core.tools import tool
from concurrent.futures import ThreadPoolExecutor, as_completed
from Configurations import Configuration
from dataclasses import asdict
from json_parser_utils import robust_json_parse, create_fallback_dict


def _extract_llm_usage(output_obj: Any) -> dict:
    """
    ä» LangChain LLM è¾“å‡ºå¯¹è±¡ä¸­å°½å¯èƒ½æå– token ç”¨é‡ã€‚
    ä¼˜å…ˆè¯»å– usage_metadataï¼Œå…¶æ¬¡è¯»å– response_metadata.token_usageã€‚
    è¿”å›å­—å…¸åŒ…å« inputã€outputã€total ä¸‰ä¸ªå­—æ®µï¼Œå‡ä¸º intã€‚
    """
    input_tokens = 0
    output_tokens = 0
    total_tokens = 0
    try:
        # LangChain >=0.2: usage_metadata é€šå¸¸åŒ…å« input_tokens/output_tokens/total_tokens
        usage_meta = getattr(output_obj, "usage_metadata", None) or {}
        if isinstance(usage_meta, dict):
            input_tokens = int(usage_meta.get("input_tokens", 0) or 0)
            output_tokens = int(usage_meta.get("output_tokens", 0) or 0)
            total_tokens = int(usage_meta.get("total_tokens", input_tokens + output_tokens) or 0)
        # å…¼å®¹éƒ¨åˆ†é©±åŠ¨æŠŠ token_usage æ”¾åœ¨ response_metadata é‡Œ
        if (input_tokens + output_tokens) == 0:
            resp_meta = getattr(output_obj, "response_metadata", None) or {}
            token_usage = {}
            if isinstance(resp_meta, dict):
                token_usage = resp_meta.get("token_usage") or resp_meta.get("usage") or {}
            if isinstance(token_usage, dict):
                # å…¼å®¹ä¸åŒå­—æ®µå‘½å
                input_tokens = int(token_usage.get("input_tokens", token_usage.get("prompt_tokens", 0)) or 0)
                output_tokens = int(token_usage.get("output_tokens", token_usage.get("completion_tokens", 0)) or 0)
                total_tokens = int(token_usage.get("total_tokens", input_tokens + output_tokens) or 0)
    except Exception:
        # é™é»˜å¤±è´¥ï¼Œè¿”å›0
        pass
    return {"input": input_tokens, "output": output_tokens, "total": total_tokens}

def _fallback_evaluation(action: str, response: str, current_stage: str, emotional_state,
                         customer_intent_level: str) -> float:
    """
    åŸºäºè§„åˆ™çš„å…œåº•è¯„ä¼°æœºåˆ¶ï¼Œå½“è¯„ä¼°æ¨¡å‹å¤±è´¥æ—¶ä½¿ç”¨
    """
    score = 0.5  # é»˜è®¤ä¸­ç­‰åˆ†æ•°

    # æ£€æŸ¥å›å¤æ˜¯å¦è¿‡çŸ­æˆ–è¿‡é•¿
    if len(response.strip()) < 3:
        return 0.2  # è¿‡çŸ­å›å¤
    if len(response) > 500:
        return 0.4  # è¿‡é•¿å›å¤

    # æ ¹æ®é˜¶æ®µè°ƒæ•´åŸºç¡€åˆ†æ•°
    stage_scores = {
        "initial_contact": {"greeting": 0.8, "rapport_building": 0.7},
        "ice_breaking": {"rapport_building": 0.8, "needs_analysis": 0.6},
        "subtle_expertise": {"value_display": 0.8, "needs_analysis": 0.7},
        "pain_point_mining": {"needs_analysis": 0.8, "pain_point_test": 0.7},
        "solution_visualization": {"value_pitch": 0.8, "value_display": 0.7},
        "natural_invitation": {"active_close": 0.8, "value_pitch": 0.6}
    }

    if current_stage in stage_scores and action in stage_scores[current_stage]:
        score = stage_scores[current_stage][action]

    # æ ¹æ®æƒ…æ„ŸçŠ¶æ€è°ƒæ•´
    trust_level = emotional_state.trust_level if emotional_state else 0.5
    comfort_level = emotional_state.comfort_level if emotional_state else 0.5

    # ä¿¡ä»»åº¦ä½æ—¶ï¼Œä¼˜å…ˆå…³ç³»å»ºç«‹
    if trust_level < 0.3:
        if action in ["rapport_building", "greeting"]:
            score += 0.1
        elif action in ["active_close", "value_pitch"]:
            score -= 0.2

    # èˆ’é€‚åº¦ä½æ—¶ï¼Œé¿å…å‹åŠ›è¿‡å¤§çš„åŠ¨ä½œ
    if comfort_level < 0.3:
        if action in ["stress_response", "rapport_building"]:
            score += 0.1
        elif action in ["active_close"]:
            score -= 0.1

    # æ ¹æ®å®¢æˆ·æ„å‘è°ƒæ•´
    if customer_intent_level == "high" and action == "active_close":
        score += 0.1
    elif customer_intent_level == "low" and action == "active_close":
        score -= 0.2


    # æ£€æŸ¥å›å¤æ˜¯å¦ç›´æ¥æä¾›äº†ä¿¡æ¯è€Œä¸æ˜¯ç»§ç»­æé—®
    if action == "value_display":
        if any(keyword in response for keyword in ["é¡¹ç›®", "æ–¹æ³•", "ä»·æ ¼", "æ•ˆæœ", "å¯ä»¥"]):
            score += 0.15  # å¥–åŠ±æä¾›ä¿¡æ¯çš„å›å¤
    elif action == "needs_analysis":
        if any(keyword in response for keyword in ["ä»€ä¹ˆ", "æ€ä¹ˆ", "å“ªç§", "ä¸ºä»€ä¹ˆ"]):
            score -= 0.1  # é™ä½ç»§ç»­æé—®çš„å›å¤åˆ†æ•°

    return max(0.1, min(1.0, score))  # ç¡®ä¿åœ¨åˆç†èŒƒå›´å†…

def _generate_and_evaluate_action(
        action: str,
        state_data: dict,  # ä¼ é€’æ•´ä¸ªçŠ¶æ€ä»¥è·å–æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡
):
    """
    ä¸ºå•ä¸ªåŠ¨ä½œç”Ÿæˆå¹¶è¯„ä¼°å›å¤ã€‚è¿™æ˜¯ä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œç”¨äºå¹¶è¡Œæ‰§è¡Œã€‚
    """

    # ä» state ä¸­è§£æ„æ‰€éœ€å˜é‡
    messages = state_data.get("long_term_messages", [])
    agent_temperature = state_data.get("agent_temperature", 0.5)
    debug_info = state_data.get("debug_info", DebugInfo())
    current_stage = debug_info.current_stage if debug_info and debug_info.current_stage else ""
    emotional_state = state_data.get("emotional_state", EmotionalState())
    customer_intent_level = state_data.get("customer_intent_level", "low")
    
    print(f"[DEBUG] [{action}] è§£æ„å˜é‡å®Œæˆ:")
    print(f"[DEBUG] [{action}] - messages æ•°é‡: {len(messages)}")
    print(f"[DEBUG] [{action}] - agent_temperature: {agent_temperature}")
    print(f"[DEBUG] [{action}] - current_stage: {current_stage}")
    print(f"[DEBUG] [{action}] - customer_intent_level: {customer_intent_level}")

    # æ£€æŸ¥æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
    user_messages = []
    for msg in reversed(messages):
        if isinstance(msg, HumanMessage):
            user_messages.insert(0, msg.content)  # ä¿æŒåŸæœ‰é¡ºåº
        elif isinstance(msg, AIMessage):
            break  # é‡åˆ°AIæ¶ˆæ¯å°±åœæ­¢
    last_user_message = "".join(user_messages) if user_messages else ""

    # é¦–å…ˆç”Ÿæˆå›å¤
    try:
        print(f"[DEBUG] [{action}] åˆ›å»º LLM å®ä¾‹...")
        
        # ä»…ä½¿ç”¨è¿è¡Œæ—¶é…ç½®ï¼ˆä¼˜å…ˆ assistant çº§åˆ«ï¼Œå†å›é€€å…¨å±€ï¼‰
        from agents.persona_config.config_manager import config_manager
        try:
            from agents.persona_config.multi_assistant_config_manager import (
                multi_assistant_config_manager,
            )
        except Exception:
            multi_assistant_config_manager = None

        assistant_id_in_state = state_data.get("assistant_id")
        config_dict = {}
        if multi_assistant_config_manager and assistant_id_in_state:
            try:
                config_dict = (
                    multi_assistant_config_manager.get_assistant_config(
                        assistant_id_in_state
                    )
                    or {}
                )
            except Exception:
                config_dict = {}
        if not config_dict:
            config_dict = config_manager.get_config() or {}
        # å­—æ®µå…¼å®¹ä¸åˆ«åå›å¡«ï¼Œé¿å…æ¨¡æ¿formatæ—¶ KeyError
        try:
            # çº æ­£å¸¸è§æ‹¼å†™
            if "industry_konwledge" in config_dict and "industry_knowledge" not in config_dict:
                config_dict["industry_knowledge"] = config_dict.get("industry_konwledge")
            # åˆ«ååŒæ­¥
            if "industry" not in config_dict and "industry_knowledge" in config_dict:
                config_dict["industry"] = config_dict["industry_knowledge"]
            if "industry_knowledge" not in config_dict and "industry" in config_dict:
                config_dict["industry_knowledge"] = config_dict["industry"]
            # è¡¥å…¨é»˜è®¤åŠ©æ‰‹æ¨¡æ¿é‡Œå¸¸ç”¨å­—æ®µï¼Œé¿å… KeyError
            defaults = {
                "agent_nickname": config_dict.get("agent_name", "{{}}"),
                "agent_birthday": config_dict.get("agent_birthday", "1998-01-01"),
                "agent_goal": "é‚€çº¦åˆ°åº—",
                "agent_side_goal": "æ”¶é›†å®¢æˆ·åé¦ˆ",
            }
            for k, v in defaults.items():
                config_dict.setdefault(k, v)
        except Exception:
            pass

        model_provider = config_dict.get("model_provider", "openrouter")
        model_name = config_dict.get(
            "model_name", config_dict.get("generation_model", "openai/gpt-5-chat")
        )
        agent_temperature = float(
            config_dict.get("agent_temperature", agent_temperature)
        )
        
        from llm import create_llm
        response_sampler = create_llm(
            model_provider=model_provider,
            model_name=model_name,
            temperature=agent_temperature
        )
        print(f"[DEBUG] [{action}] LLM å®ä¾‹åˆ›å»ºæˆåŠŸ: {type(response_sampler)}")

        # ä½¿ç”¨ä¸ä¹‹å‰ç›¸åŒçš„é€»è¾‘ï¼šåŠ è½½ prompt æ¨¡æ¿å¹¶æ ¼å¼åŒ–
        from prompts.loader import load_prompt

        # åŠ è½½å¯¹åº”çš„ prompt æ¨¡æ¿
        print(f"[DEBUG] [{action}] åŠ è½½ prompt æ¨¡æ¿...")
        # ä½¿ç”¨é…ç½®ä¸­çš„base_context_prompt
        custom_base_context = config_dict.get("base_context_prompt") or state_data.get("base_context")
        prompt_template = load_prompt(action, custom_base_context=custom_base_context)
        print(f"[DEBUG] [{action}] prompt æ¨¡æ¿åŠ è½½æˆåŠŸï¼Œé•¿åº¦: {len(prompt_template)}")

        # æ ¼å¼åŒ–å¯¹è¯å†å²
        # æ ¼å¼åŒ–å¯¹è¯å†å²
        def _format_messages(messages: List[Any]) -> str:
            """å°† LangChain BaseMessage å¯¹è±¡çš„åˆ—è¡¨æ ¼å¼åŒ–ä¸ºå•ä¸ªå­—ç¬¦ä¸²ã€‚"""
            if not messages:
                return "ï¼ˆæ— å†å²è®°å½•ï¼‰"

            formatted_string = ""
            for message in messages:
                # ä¿®å¤ï¼šä½¿ç”¨ __class__.__name__ æ¥åˆ¤æ–­æ¶ˆæ¯ç±»å‹
                message_type = message.__class__.__name__
                if message_type == "HumanMessage":
                    role = "å®¢æˆ·"
                elif message_type == "AIMessage":
                    role = "AI"
                elif message_type == "SystemMessage":
                    role = "ç³»ç»Ÿ"
                else:
                    role = "å…¶ä»–"
                
                # è·å–æ¶ˆæ¯å†…å®¹
                content = message.content
                
                # å®‰å…¨æå–send_styleå­—æ®µï¼Œæä¾›é»˜è®¤å€¼
                send_style = message.additional_kwargs.get('send_style', 'text')
                content = f"[{send_style}] {content}"
                
                formatted_string += f"{role}: {content}\n"
            return formatted_string.strip()

        # æ ¼å¼åŒ– promptï¼Œå°†å¯¹è¯å†å²æ’å…¥æ¨¡æ¿
        
        # è°ƒè¯•è¾“å‡ºï¼šæ‰“å°é…ç½®ä¿¡æ¯
        print(f"[DEBUG] [{action}] é…ç½®ä¿¡æ¯:")
        print(f"[DEBUG] [{action}] - agent_name: {config_dict.get('agent_name', 'N/A')}")
        print(f"[DEBUG] [{action}] - company_name: {config_dict.get('company_name', 'N/A')}")
        print(f"[DEBUG] [{action}] - service_time: {config_dict.get('service_time', 'N/A')}")
        base_context = config_dict.get('base_context_prompt', '')
        print(f"[DEBUG] [{action}] - base_context_prompt: {base_context[:100]}..." if base_context else "[DEBUG] - base_context_prompt: None")
        
        # ç”Ÿæˆæ—¶é—´ç›¸å…³çš„è½»å¾®æç¤ºï¼ˆåªåœ¨ç‰¹å®šåŠ¨ä½œä¸‹ï¼‰
        import datetime
        from zoneinfo import ZoneInfo
        current_time_info = datetime.datetime.now(ZoneInfo("Asia/Shanghai")).strftime("%Y-%m-%d %H:%M:%S")
        now = datetime.datetime.now(ZoneInfo("Asia/Shanghai"))
        current_hour = now.hour
        
        # åªåœ¨ç‰¹å®šåŠ¨ä½œä¸‹æ·»åŠ æ—¶é—´ç›¸å…³æŒ‡ä»¤ï¼Œå¹¶ä½œä¸ºè½»å¾®çš„è¡¥å……ä¿¡æ¯
        time_context = ""
        if action in ["greeting", "rapport_building"]:
            if 6 <= current_hour < 11:
                time_context = "\n\nå¶å°”æåŠå³å¯ï¼Œä¸è¦ä¸€ç›´æåŠæ—¶é—´æ¦‚å¿µï¼šç°åœ¨æ˜¯æ—©ä¸Šæ—¶é—´ï¼Œå¯ä»¥ç¨å¾®æœ‰æ´»åŠ›ä¸€äº›ã€‚"
            elif 11 <= current_hour < 17:
                time_context = "\n\nå¶å°”æåŠå³å¯ï¼Œä¸è¦ä¸€ç›´æåŠæ—¶é—´æ¦‚å¿µï¼šç°åœ¨æ˜¯ä¸‹åˆæ—¶é—´ï¼Œå¯ä»¥é€‚å½“å…³å¿ƒå¯¹æ–¹ã€‚"
            elif 17 <= current_hour < 22:
                time_context = "\n\nå¶å°”æåŠå³å¯ï¼Œä¸è¦ä¸€ç›´æåŠæ—¶é—´æ¦‚å¿µï¼šç°åœ¨æ˜¯å‚æ™šæ—¶é—´ï¼Œè¯­æ°”å¯ä»¥ç¨å¾®è½»æ¾ä¸€äº›ã€‚"
            else:  # 22:00 - 06:00
                time_context = "\n\nå¶å°”æåŠå³å¯ï¼Œä¸è¦ä¸€ç›´æåŠæ—¶é—´æ¦‚å¿µï¼šç°åœ¨æ˜¯æ·±å¤œæ—¶é—´ï¼Œæ³¨æ„å…³å¿ƒå¯¹æ–¹ä¼‘æ¯ã€‚å¦å¤–åˆ°æ™šä¸Šçš„æ—¶å€™ï¼Œä½ ä¼šç¨å¾®è¯±äººä¸€äº›ï¼Œä½ ä¼šç¨å¾®å¼•è¯±ä¸€ä¸‹å®¢äººçš„äº²å¯†å…³ç³»ï¼Œä½†æ˜¯å¾ˆæŒ‘é€—ï¼Œå¹¶ä¸”ä¸ç»æ„ï¼Œå‡è£…çŸœæŒçš„æŒ‘é€—"
        
        # è°ƒè¯•è¾“å‡ºï¼šæ‰“å°æ—¶é—´ç›¸å…³æŒ‡ä»¤
        print(f"[DEBUG] [{action}] æ—¶é—´è¡¥å……æç¤º: {time_context}")
        
        # å°†æ—¶é—´æç¤ºæ•´åˆåˆ°ä¸»promptä¸­ï¼Œè€Œä¸æ˜¯ä½œä¸ºç‹¬ç«‹æ¶ˆæ¯
        base_prompt = prompt_template.format(
            message_history=_format_messages(messages),
            **config_dict
        )

        # å°†æ—¶é—´æç¤ºä½œä¸ºä¸»promptçš„è¡¥å……éƒ¨åˆ†
        prompt = base_prompt + time_context

        # ===== æ–°å¢ï¼šå¤šåª’ä½“å†…å®¹æ„ŸçŸ¥ =====
        # æ£€æŸ¥æ˜¯å¦æœ‰å³å°†å‘é€çš„å¤šåª’ä½“å†…å®¹ï¼Œå‘ŠçŸ¥AIä»¥ä¾¿ç”Ÿæˆåè°ƒçš„å›å¤
        multimedia_context = ""

        # æ£€æŸ¥æ˜¯å¦æœ‰é€‰ä¸­çš„ç´ æå³å°†å‘é€
        selected_image = state_data.get("selected_image")
        if selected_image and isinstance(selected_image, dict):
            material_name = selected_image.get("name", "ç´ æ")
            material_type = selected_image.get("materialType", 2)  # ä½¿ç”¨æ–°çš„materialTypeå­—æ®µ

            # æ ¹æ®ç´ æç±»å‹ç”Ÿæˆä¸åŒçš„æç¤ºè¯
            material_type_names = {
                2: "å›¾ç‰‡", 3: "è§†é¢‘", 4: "å¡ç‰‡é“¾æ¥", 5: "å¡ç‰‡", 6: "è¯­éŸ³", 7: "æ–‡ä»¶"
            }
            type_name = material_type_names.get(material_type, "ç´ æ")

            if material_type == 2:  # å›¾ç‰‡
                multimedia_context += f"\n\nã€ç³»ç»Ÿæç¤ºã€‘ä½ å°†åŒæ—¶å‘é€ä¸€å¼ å›¾ç‰‡ç»™ç”¨æˆ·ï¼Œå›¾ç‰‡åç§°ä¸ºï¼š{material_name}ã€‚è¯·åœ¨å›å¤ä¸­è‡ªç„¶åœ°æåŠè¿™å¼ å›¾ç‰‡ï¼Œæ¯”å¦‚å¯ä»¥è¯´'æˆ‘å‘äº†ä¸€å¼ {material_name}ç»™ä½ çœ‹çœ‹'æˆ–'è¿™æ˜¯æˆ‘ä»¬çš„{material_name}'ï¼Œè®©å›å¤å†…å®¹ä¸å›¾ç‰‡å½¢æˆè‰¯å¥½çš„é…åˆã€‚"
            elif material_type == 3:  # è§†é¢‘
                multimedia_context += f"\n\nã€ç³»ç»Ÿæç¤ºã€‘ä½ å°†åŒæ—¶å‘é€ä¸€ä¸ªè§†é¢‘ç»™ç”¨æˆ·ï¼Œè§†é¢‘åç§°ä¸ºï¼š{material_name}ã€‚è¯·åœ¨å›å¤ä¸­è‡ªç„¶åœ°å¼•å¯¼ç”¨æˆ·è§‚çœ‹è§†é¢‘ï¼Œæ¯”å¦‚å¯ä»¥è¯´'æˆ‘å‘äº†ä¸€ä¸ª{material_name}çš„è§†é¢‘ç»™ä½ 'æˆ–'ä½ å¯ä»¥çœ‹çœ‹è¿™ä¸ª{material_name}çš„æ¼”ç¤ºè§†é¢‘'ï¼Œè®©å›å¤å†…å®¹ä¸è§†é¢‘å†…å®¹åè°ƒä¸€è‡´ã€‚"
            elif material_type == 4:  # å¡ç‰‡é“¾æ¥
                multimedia_context += f"\n\nã€ç³»ç»Ÿæç¤ºã€‘ä½ å°†åŒæ—¶å‘é€ä¸€ä¸ªå¡ç‰‡é“¾æ¥ç»™ç”¨æˆ·ï¼Œå¡ç‰‡åç§°ä¸ºï¼š{material_name}ã€‚è¯·åœ¨å›å¤ä¸­è‡ªç„¶åœ°æåŠè¿™ä¸ªé“¾æ¥ï¼Œæ¯”å¦‚å¯ä»¥è¯´'æˆ‘å‘äº†ä¸€ä¸ª{material_name}çš„è¯¦ç»†ä»‹ç»ç»™ä½ 'æˆ–'ä½ å¯ä»¥ç‚¹å‡»æŸ¥çœ‹{material_name}çš„è¯¦ç»†ä¿¡æ¯'ï¼Œå¼•å¯¼ç”¨æˆ·ç‚¹å‡»é“¾æ¥ã€‚"
            elif material_type == 5:  # å¡ç‰‡
                multimedia_context += f"\n\nã€ç³»ç»Ÿæç¤ºã€‘ä½ å°†åŒæ—¶å‘é€ä¸€ä¸ªå¡ç‰‡ç»™ç”¨æˆ·ï¼Œå¡ç‰‡åç§°ä¸ºï¼š{material_name}ã€‚è¯·åœ¨å›å¤ä¸­è‡ªç„¶åœ°é…åˆè¿™ä¸ªå¡ç‰‡ï¼Œæ¯”å¦‚å¯ä»¥è¯´'æˆ‘æ•´ç†äº†ä¸€ä¸ª{material_name}ç»™ä½ 'æˆ–'è¿™æ˜¯{material_name}çš„ç›¸å…³ä¿¡æ¯'ï¼Œè®©å›å¤ä¸å¡ç‰‡å†…å®¹å½¢æˆäº’è¡¥,ä½†æ˜¯ä¸è¦æåŠâ€œå¡ç‰‡â€äºŒå­—æœ¬èº«ã€‚"
            elif material_type == 6:  # è¯­éŸ³
                multimedia_context += f"\n\nã€ç³»ç»Ÿæç¤ºã€‘ä½ å°†åŒæ—¶å‘é€ä¸€ä¸ªè¯­éŸ³æ–‡ä»¶ç»™ç”¨æˆ·ï¼Œè¯­éŸ³åç§°ä¸ºï¼š{material_name}ã€‚è¯·åœ¨å›å¤ä¸­æåŠè¿™ä¸ªè¯­éŸ³ï¼Œæ¯”å¦‚å¯ä»¥è¯´'æˆ‘å½•åˆ¶äº†ä¸€ä¸ª{material_name}çš„è¯­éŸ³ç»™ä½ 'æˆ–'ä½ å¯ä»¥å¬å¬è¿™ä¸ª{material_name}çš„è¯­éŸ³ä»‹ç»'ã€‚"
            elif material_type == 7:  # æ–‡ä»¶
                multimedia_context += f"\n\nã€ç³»ç»Ÿæç¤ºã€‘ä½ å°†åŒæ—¶å‘é€ä¸€ä¸ªæ–‡ä»¶ç»™ç”¨æˆ·ï¼Œæ–‡ä»¶åç§°ä¸ºï¼š{material_name}ã€‚è¯·åœ¨å›å¤ä¸­æåŠè¿™ä¸ªæ–‡ä»¶ï¼Œæ¯”å¦‚å¯ä»¥è¯´'æˆ‘å‘äº†ä¸€ä¸ª{material_name}çš„æ–‡ä»¶ç»™ä½ 'æˆ–'ä½ å¯ä»¥ä¸‹è½½æŸ¥çœ‹{material_name}çš„è¯¦ç»†èµ„æ–™'ã€‚"
            else:
                multimedia_context += f"\n\nã€ç³»ç»Ÿæç¤ºã€‘ä½ å°†åŒæ—¶å‘é€ä¸€ä¸ª{type_name}ç»™ç”¨æˆ·ï¼Œåç§°ä¸ºï¼š{material_name}ã€‚è¯·åœ¨å›å¤ä¸­è‡ªç„¶åœ°æåŠè¿™ä¸ª{type_name}ï¼Œè®©å›å¤å†…å®¹ä¸ä¹‹åè°ƒä¸€è‡´ã€‚"

        # æ£€æŸ¥æ˜¯å¦æœ‰è¯­éŸ³å›å¤çš„æ„å›¾
        audio_reply = state_data.get("audio_reply")
        if audio_reply:
            multimedia_context += f"\n\nã€ç³»ç»Ÿæç¤ºã€‘ä½ å½“å‰çš„åœºæ™¯æ˜¯åœ¨å¾®ä¿¡ä¸Šçš„è¯­éŸ³å›å¤ã€‚ä½ å°†è¦ä»¥è¯­éŸ³å½¢å¼å›å¤ç”¨æˆ·ï¼Œè€Œä¸æ˜¯æ–‡å­—ã€‚è¯·ç¡®ä¿å›å¤å†…å®¹é€‚åˆè¯­éŸ³æ’­æ”¾ï¼Œè¯­æ°”è‡ªç„¶ã€å£è¯­åŒ–ã€‚é¿å…å‡ºç°â€œæˆ‘ç°åœ¨æ²¡æ³•å‘é€è¯­éŸ³â€â€œæˆ‘ç­‰ä¸€ä¸‹å†ç»™ä½ å‘è¯­éŸ³â€è¿™æ ·çš„å›å¤ã€‚"

        # å¦‚æœæœ‰è¯­éŸ³è¯†åˆ«çš„æ–‡å­—å†…å®¹ï¼Œä¹Ÿè¦è€ƒè™‘
        custom_audio_text = state_data.get("custom_audio_text", [])
        if custom_audio_text:
            # è¿‡æ»¤æ‰ç©ºå­—ç¬¦ä¸²ï¼Œåˆå¹¶æ‰€æœ‰æœ‰æ•ˆçš„è¯­éŸ³è¯†åˆ«å†…å®¹
            valid_audio_texts = [text for text in custom_audio_text if text and text.strip()]
            if valid_audio_texts:
                combined_audio_text = "\n".join(valid_audio_texts)
                multimedia_context += f"\n\nã€ç³»ç»Ÿæç¤ºã€‘ç”¨æˆ·é€šè¿‡è¯­éŸ³å‘é€äº†æ¶ˆæ¯ï¼Œè¯­éŸ³è¯†åˆ«å†…å®¹ä¸ºï¼š{combined_audio_text[:100]}... è¯·ç»“åˆè¯­éŸ³è¯†åˆ«å†…å®¹è¿›è¡Œå›å¤ã€‚"

        # æ£€æŸ¥æ˜¯å¦æœ‰ç´ æé€‰æ‹©å¤±è´¥çš„æƒ…å†µ
        material_failure = state_data.get("need_material_failure_response")
        if material_failure:
            failure_reason = state_data.get("material_selection_failure_reason", "unknown")
            if failure_reason == "no_suitable_material":
                multimedia_context += "\n\nã€ç³»ç»Ÿæç¤ºã€‘ç”¨æˆ·è¯·æ±‚å‘é€é™„ä»¶ï¼Œä½†å½“å‰æ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„ç´ æã€‚è¯·åœ¨å›å¤ä¸­ç¤¼è²Œåœ°è¯´æ˜æš‚æ—¶æ— æ³•æä¾›ç›¸åº”çš„é™„ä»¶ï¼Œå¹¶å¯ä»¥å»ºè®®ç”¨æˆ·ç”¨å…¶ä»–æ–¹å¼æè¿°éœ€æ±‚ï¼Œæˆ–è€…è¡¨ç¤ºä¼šå°½å¿«è¡¥å……ç›¸å…³ææ–™ã€‚"
            else:
                multimedia_context += "\n\nã€ç³»ç»Ÿæç¤ºã€‘é™„ä»¶å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸ã€‚è¯·åœ¨å›å¤ä¸­å®‰æ…°ç”¨æˆ·ï¼Œå¹¶è¡¨ç¤ºæš‚æ—¶è¿˜ä¸èƒ½å‘é€ã€‚æ­£åœ¨å¤„ç†è¿™ä¸ªé—®é¢˜ã€‚"

        # å°†å¤šåª’ä½“ä¸Šä¸‹æ–‡æ·»åŠ åˆ°ä¸»prompt
        if multimedia_context:
            prompt += multimedia_context
            print(f"[MULTIMEDIA] [{action}] æ·»åŠ å¤šåª’ä½“ä¸Šä¸‹æ–‡åˆ°prompt: {multimedia_context.strip()}")
        
        # è°ƒè¯•è¾“å‡ºï¼šæ‰“å°æœ€ç»ˆç”Ÿæˆçš„prompt
        print(f"[DEBUG] [{action}] base + time (å‰200å­—ç¬¦):")
        print(f"[DEBUG] [{action}] {prompt[:200]}...")
        
        messages_for_sampler = [SystemMessage(content=prompt)]
        
        # æ·»åŠ å½“å‰æ—¶é—´ä¿¡æ¯ï¼ˆä½œä¸ºå•ç‹¬çš„ç³»ç»Ÿæ¶ˆæ¯ï¼Œä½†ä¼˜å…ˆçº§è¾ƒä½ï¼‰
        messages_for_sampler.append(SystemMessage(content="å½“å‰æ—¶é—´æ˜¯:"+current_time_info))
        
        TOOL_NAME_MAP={"search": "è”ç½‘æœç´¢"}
        tool_results=state_data.get("tool_results", {})
        used_tools=state_data.get("used_tools", {})
        
        for tool_info, result_info in zip(used_tools, tool_results):
            tool_name = tool_info.get("tool")
            reason = tool_info.get("reason", "")
            result = result_info.get("result")

            if result:
                tool_label = TOOL_NAME_MAP.get(tool_name, tool_name)
                system_msg = (
                    f"å®¢æˆ·å¯èƒ½å…³å¿ƒï¼š{reason}ã€‚\n"
                    f"è°ƒç”¨å·¥å…·ã€{tool_label}ã€‘å¾—åˆ°çš„ç»“æœæ˜¯ï¼š\n{result}\n\n"
                    f"æ ¹æ®å·¥å…·è°ƒç”¨çš„ç»“æœï¼Œç”¨ä¸€å¥è¿ç»­çš„è¯æ¥å›å¤å®¢æˆ·ï¼š\n"
                    f"è®°ä½ï¼šæ— è®ºä¹‹å‰è°ƒç”¨äº†ä»€ä¹ˆå·¥å…·ï¼Œä½ ç°åœ¨è¦ç»§ç»­ç”¨åŸæœ‰çš„å£å»å›ç­”ç”¨æˆ·"
                    f"- å¯ä»¥åªå›å¤å¾ˆçŸ­çš„è¯ï¼Œä¹Ÿå¯ä»¥ç¨é•¿ä¸€ç‚¹\n"
                    f"- è®°ä½ä½ çš„èº«ä»½å’Œå·¥ä½œï¼Œåˆ«è·‘åäº†ï¼Œä½ è¦è®©äººæ„Ÿè§‰ä¸åˆ°åœ°å°†è¯é¢˜å¼•å¯¼å›ä½ çš„ä¸šåŠ¡èŒƒå›´\n"
                )
                messages_for_sampler.append(SystemMessage(content=system_msg))
        
        # è°ƒè¯•è¾“å‡ºï¼šæ‰“å°æ‰€æœ‰å‘é€ç»™æ¨¡å‹çš„æ¶ˆæ¯
        print(f"[DEBUG] [{action}] å‘é€ç»™æ¨¡å‹çš„æ‰€æœ‰æ¶ˆæ¯:")
        for i, msg in enumerate(messages_for_sampler):
            # ç¡®ä¿contentæ˜¯å­—ç¬¦ä¸²ç±»å‹å†è¿›è¡Œåˆ‡ç‰‡
            content = msg.content
            if isinstance(content, str):
                content_preview = content[:100]
            else:
                content_preview = str(content)[:100]
            print(f"[DEBUG] [{action}] æ¶ˆæ¯ {i+1}: {content_preview}...")
        
        # ç¡®ä¿æœ‰ä¸€æ¡æ˜ç¡®çš„äººç±»æŒ‡ä»¤ï¼Œé¿å…éƒ¨åˆ†æ¨¡å‹åœ¨ä»…æœ‰ç³»ç»Ÿæ¶ˆæ¯æ—¶è¿”å›ç©ºä¸²
        messages_for_sampler.append(HumanMessage(content='è¯·æ ¹æ®ä¸Šè¿°è¦æ±‚ï¼Œä¸¥æ ¼ä»…è¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼š{"response": "ä½ çš„è‡ªç„¶å›åº”"}ã€‚ä¸è¦è¾“å‡ºå…¶ä»–è¯´æ˜ã€‚'))
        
        # æ ¹æ® action æä¾›å¯¹åº”çš„ fallback å›å¤ - ç§»åˆ°è¿™é‡Œï¼Œè¿™æ˜¯ä¸‡ä¸å¾—å·²çš„å›å¤ï¼Œåªä¸è¿‡å¤šæ ·æ€§ä¸€ç‚¹è€Œå·²
        fallback_responses = {
            "greeting": "æ‚¨å¥½ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨ï¼Ÿ",
            "rapport_building": "æˆ‘ä»¬èŠç‚¹åˆ«çš„å§ï¼",
            "needs_analysis": "å…³äºæ‚¨çš„æƒ…å†µï¼Œèƒ½å†å¤šè¯´ä¸€ç‚¹å—ï¼Ÿ",
            "value_display": "é’ˆå¯¹æ‚¨çš„æƒ…å†µï¼Œæˆ‘ä»¬æœ‰å¾ˆå¤šä¸“ä¸šçš„è§£å†³æ–¹æ¡ˆã€‚",
            "stress_response": "æŠ±æ­‰ï¼Œæˆ‘ä»¬æ¢ä¸ªè¯é¢˜å§ã€‚",
            "pain_point_test": "æˆ‘ä»¬èŠèŠæ‚¨é‡åˆ°çš„å…·ä½“æƒ…å†µå§ï¼Ÿ",
            "value_pitch": "å…³äºæˆ‘ä»¬çš„æ–¹æ¡ˆï¼Œæ‚¨æœ€å…³å¿ƒå“ªä¸ªæ–¹é¢ï¼Ÿ",
            "active_close": "æˆ‘ä»¬ç›´æ¥è¿›å…¥ä¸‹ä¸€æ­¥å§ï¼",
            "reverse_probe": "å¯ä»¥å¤šå‘Šè¯‰æˆ‘ä¸€äº›æ‚¨çš„å…·ä½“æƒ…å†µå—ï¼Ÿ"
        }
        fallback_response = fallback_responses.get(action, "å—¯å—¯ï¼Œå¥½çš„")

        # è°ƒç”¨æ¨¡å‹ç”Ÿæˆå›å¤
        try:
            # æ˜ç¡®è¦æ±‚è¿”å› JSONï¼Œé™ä½ç©ºå“åº”æ¦‚ç‡
            response_result = response_sampler.invoke(
                messages_for_sampler,
                response_format={"type": "json_object"}
            )
            print(f"[DEBUG] [{action}] ç”Ÿæˆæ¨¡å‹è°ƒç”¨æˆåŠŸï¼Œè¿”å›ç±»å‹: {type(response_result)}")
            
            response_text = response_result.content if hasattr(response_result, 'content') else str(response_result)
            print(f"[DEBUG] [{action}] æå–çš„response_textç±»å‹: {type(response_text)}, é•¿åº¦: {len(str(response_text))}")
            # è‹¥ä»ä¸ºç©ºï¼Œæ„é€ è§„èŒƒJSONä»¥ä¾¿åç»­è§£æå…œåº•
            if not isinstance(response_text, str) or not response_text.strip():
                response_text = json.dumps({"response": fallback_response}, ensure_ascii=False)
            
        except Exception as e:
            print(f"[DEBUG] [{action}] ç”Ÿæˆæ¨¡å‹è°ƒç”¨å¤±è´¥ï¼Œé”™è¯¯ç±»å‹: {type(e)}, é”™è¯¯ä¿¡æ¯: {e}")
            print(f"[DEBUG] [{action}] é”™è¯¯è¯¦æƒ…: {str(e)}")
            import traceback
            print(f"[DEBUG] [{action}] é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            response_text = fallback_response

        # ç»Ÿè®¡ç”Ÿæˆé˜¶æ®µ token
        generation_usage = _extract_llm_usage(locals().get("response_result")) if 'response_result' in locals() else {"input": 0, "output": 0, "total": 0}

        # å®‰å…¨è§£æ JSON å“åº”
        def _safe_json_parse(response_text: str, fallback_response: str) -> str:
            """å®‰å…¨åœ°è§£æAPIè¿”å›çš„JSONå“åº”ï¼Œå¤„ç†å„ç§å¼‚å¸¸æƒ…å†µ"""
            if response_text is None:
                return fallback_response
            
            print(f"[DEBUG-ç”Ÿæˆè§£æ-{action}] åŸå§‹æ¨¡å‹å“åº”: {response_text}")
            
            # ä½¿ç”¨é²æ£’çš„JSONè§£æå·¥å…·
            fallback_dict = {"response": fallback_response}
            parsed_data = robust_json_parse(
                response_text, 
                context=f"ç”Ÿæˆå“åº”è§£æ-{action}", 
                fallback_dict=fallback_dict,
                debug=True
            )
            
            result = parsed_data.get("response", fallback_response)
            print(f"[DEBUG-ç”Ÿæˆè§£æ-{action}] è§£æç»“æœ: {result}")
            return result

        response = _safe_json_parse(response_text, fallback_response)

        # ç„¶åè¯„ä¼°å›å¤
        print(f"[DEBUG] [{action}] åˆ›å»ºè¯„ä¼°æ¨¡å‹å®ä¾‹...")
        # è¯„ä¼°æ¨¡å‹ä¹Ÿæ¥è‡ªè¿è¡Œæ—¶é…ç½®ï¼ˆè‹¥ç¼ºå¤±åˆ™å›é€€åˆ°ç”Ÿæˆæ¨¡å‹ï¼‰
        verification_model = config_dict.get("verification_model") or config_dict.get("model_name", "x-ai/grok-code-fast-1")

        # ç¡®ä¿ä½¿ç”¨openrouteræ”¯æŒçš„æ¨¡å‹ï¼Œé¿å…ä½¿ç”¨ä¸å­˜åœ¨çš„æ¨¡å‹
        if verification_model.startswith("openai/gpt-5") or verification_model == "openai/gpt-5-chat":
            verification_model = "x-ai/grok-code-fast-1"
        elif verification_model.startswith("gpt-5") and "/" not in verification_model:
            verification_model = "x-ai/grok-code-fast-1"

        feedback_sampler = create_llm(
            model_provider=model_provider,
            model_name=verification_model,
            temperature=agent_temperature
        )
        print(f"[DEBUG] [{action}] è¯„ä¼°æ¨¡å‹å®ä¾‹åˆ›å»ºæˆåŠŸ: {type(feedback_sampler)}")

        # ğŸ¯ æ”¹è¿›çš„è¯„ä¼°promptï¼šé‡ç‚¹å…³æ³¨éœ€æ±‚æ»¡è¶³

        feedback_prompt = f"""
ä½ æ˜¯å¯¹è¯è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚è¯„ä¼°è¿™ä¸ªå›å¤æ˜¯å¦åˆé€‚ã€‚

**å…³é”®åŸåˆ™ï¼šå½“å®¢æˆ·è¡¨è¾¾æ˜ç¡®éœ€æ±‚æ—¶ï¼Œä¼˜å…ˆæ»¡è¶³éœ€æ±‚è€Œä¸æ˜¯ç»§ç»­æŒ–æ˜**

**ç”¨æˆ·æœ€åè¯´ï¼š** "{last_user_message}"

**å€™é€‰å›å¤ (ç­–ç•¥: {action}):**
"{response}"

**è¯„ä¼°è¦ç‚¹ï¼š**
1. å¦‚æœç”¨æˆ·è¯´"æˆ‘æƒ³ç¾ç™½"ã€"æƒ³äº†è§£XX"ç­‰æ˜ç¡®éœ€æ±‚ï¼Œä¼˜å…ˆç»™åˆ†é«˜çš„å›å¤åº”è¯¥æ˜¯ï¼š
- ç›´æ¥æä¾›ç›¸å…³ä¿¡æ¯/é¡¹ç›®ä»‹ç» (é«˜åˆ†)
- è€Œä¸æ˜¯ç»§ç»­é—®"æ‚¨æƒ³æ”¹å–„ä»€ä¹ˆé—®é¢˜" (ä½åˆ†)

2. å¦‚æœç”¨æˆ·å·²ç»é€‰æ‹©äº†å…·ä½“é¡¹ç›®ï¼Œä¼˜å…ˆç»™åˆ†é«˜çš„å›å¤åº”è¯¥æ˜¯ï¼š
- è¿›å…¥é¢„çº¦æµç¨‹/æä¾›æ¡ˆä¾‹ (é«˜åˆ†)
- è€Œä¸æ˜¯ç»§ç»­äº†è§£éœ€æ±‚ (ä½åˆ†)
- ä¸è´´åˆå£è¯­åŒ–ã€å›å¤å†…å®¹è¶…å‡ºä¸šåŠ¡èŒƒå›´ (ä½åˆ†)

**è¯„åˆ†æ ‡å‡†:**
- 0.8-1.0: å›å¤ç›´æ¥æ»¡è¶³äº†ç”¨æˆ·éœ€æ±‚
- 0.6-0.7: å›å¤åŸºæœ¬åˆé€‚ï¼Œç•¥æœ‰åç¦»ä½†å¯æ¥å—
- 0.4-0.5: å›å¤ä¸€èˆ¬ï¼Œæ²¡æœ‰å¾ˆå¥½æ»¡è¶³éœ€æ±‚
- 0.2-0.3: å›å¤åç¦»äº†ç”¨æˆ·æ„å›¾
- 0.0-0.1: å›å¤å®Œå…¨ä¸åˆé€‚




JSONæ ¼å¼: {{"score": æ•°å€¼, "reasoning": "ç®€çŸ­ç†ç”±"}}
"""
        try:
            raw_feedback = feedback_sampler.invoke(
                [HumanMessage(content=feedback_prompt)],
                response_format={"type": "json_object"}
            )
            print(f"[DEBUG] [{action}] è¯„ä¼°æ¨¡å‹è°ƒç”¨æˆåŠŸï¼Œè¿”å›ç±»å‹: {type(raw_feedback)}")
            
        except Exception as e:
            print(f"[DEBUG] [{action}] è¯„ä¼°æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼Œé”™è¯¯ç±»å‹: {type(e)}, é”™è¯¯ä¿¡æ¯: {e}")
            print(f"[DEBUG] [{action}] é”™è¯¯è¯¦æƒ…: {str(e)}")
            import traceback
            print(f"[DEBUG] [{action}] é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            
            # ä½¿ç”¨è§„åˆ™è¯„ä¼°ä½œä¸ºå…œåº•
            score = _fallback_evaluation(action, response, current_stage, emotional_state, customer_intent_level or "low")
            reasoning = f"è¯„ä¼°æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™è¯„ä¼°: {e}"
            evaluated_response = {
                "action": action,
                "response": response,
                "score": score,
                "reasoning": reasoning
            }
            monologue_entry = f"  - [{action}] ç”Ÿæˆå›å¤: '{response[:30]}...' -> è¯„ä¼°å¾—åˆ†: {score} (åŸå› : {reasoning})"
            # è¯„ä¼°å¤±è´¥æ—¶ï¼Œæœ¬é˜¶æ®µ token è®°ä¸º0ï¼Œä»…è¿”å›ç”Ÿæˆé˜¶æ®µçš„ç”¨é‡
            round_usage = {
                "input_tokens": generation_usage.get("input", 0),
                "output_tokens": generation_usage.get("output", 0),
                "total_tokens": generation_usage.get("total", 0),
            }
            return evaluated_response, monologue_entry, round_usage

        # è¯„ä¼°é˜¶æ®µ token ç»Ÿè®¡
        evaluation_usage = _extract_llm_usage(locals().get("raw_feedback")) if 'raw_feedback' in locals() else {"input": 0, "output": 0, "total": 0}

        # ä½¿ç”¨é²æ£’çš„JSONè§£æå·¥å…·
        raw_feedback_str = raw_feedback.content if hasattr(raw_feedback, 'content') else str(raw_feedback)
        
        print(f"[DEBUG-è¯„ä¼°-{action}] åŸå§‹æ¨¡å‹å“åº”: {raw_feedback_str}")
        
        fallback_dict = {"score": 0.5, "reasoning": "è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è¯„åˆ†"}
        feedback_data = robust_json_parse(
            raw_feedback_str, 
            context=f"è¯„ä¼°-{action}", 
            fallback_dict=fallback_dict,
            debug=True
        )
        
        print(f"[DEBUG-è¯„ä¼°-{action}] è§£æç»“æœ: {feedback_data}")

        score = float(feedback_data.get("score", 0.5))  # é»˜è®¤ç»™ä¸­ç­‰åˆ†
        reasoning = feedback_data.get("reasoning", "è¯„ä¼°æˆåŠŸ")

        # ç¡®ä¿åˆ†æ•°åœ¨åˆç†èŒƒå›´å†…
        score = max(0.0, min(1.0, score))

    except Exception as eval_error:
        # ğŸ›¡ï¸ å¼ºåŒ–å…œåº•ç­–ç•¥ï¼šåŸºäºè§„åˆ™çš„å¿«é€Ÿè¯„ä¼°
        if 'response' not in locals():
            # å¦‚æœç”Ÿæˆå›å¤å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å›å¤
            response = ""
        score = _fallback_evaluation(action, response, current_stage, emotional_state, customer_intent_level or "low")
        reasoning = f"ç”Ÿæˆæˆ–è¯„ä¼°å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™è¯„ä¼°: {eval_error}"
    evaluated_response = {
        "action": action,
        "response": response,
        "score": score,
        "reasoning": reasoning
    }
    monologue_entry = f"  - [{action}] ç”Ÿæˆå›å¤: '{response[:30]}...' -> è¯„ä¼°å¾—åˆ†: {score} (åŸå› : {reasoning})"
    # æ±‡æ€»å½“æ¬¡ï¼ˆç”Ÿæˆ + è¯„ä¼°ï¼‰token ç”¨é‡
    try:
        round_usage = {
            "input_tokens": int((generation_usage.get("input", 0)) + (locals().get("evaluation_usage", {}).get("input", 0))),
            "output_tokens": int((generation_usage.get("output", 0)) + (locals().get("evaluation_usage", {}).get("output", 0))),
            "total_tokens": int((generation_usage.get("total", 0)) + (locals().get("evaluation_usage", {}).get("total", 0))),
        }
    except Exception:
        round_usage = {"input_tokens": 0, "output_tokens": 0, "total_tokens": 0}
    return evaluated_response, monologue_entry, round_usage

@tool
def generate_and_evaluate_node(state_data: dict):
    """
    å¹¶è¡Œåœ°ä¸ºæ¯ä¸ªå€™é€‰åŠ¨ä½œç”Ÿæˆå›å¤å¹¶è·å–åé¦ˆã€‚
    """
    debug_info = state_data.get("debug_info", DebugInfo())
    internal_monologue = debug_info.internal_monologue if debug_info and debug_info.internal_monologue else []
    candidate_actions = state_data.get("candidate_actions", [])

    print(f"[DEBUG] candidate_actions: {candidate_actions}")
    print(f"[DEBUG] candidate_actions æ•°é‡: {len(candidate_actions)}")

    evaluated_responses = []
    new_monologue = list(internal_monologue)
    # ç´¯è®¡å½“è½®æ‰€æœ‰å€™é€‰åŠ¨ä½œè°ƒç”¨çš„ token
    round_input_tokens = 0
    round_output_tokens = 0
    round_total_tokens = 0

    max_concurrent_requests = min(3, len(candidate_actions) or 1)

    # ä½¿ç”¨ ThreadPoolExecutor æ¥å¤„ç†å¼‚æ­¥å‡½æ•°
    print(f"[DEBUG] åˆ›å»º ThreadPoolExecutor...")
    with ThreadPoolExecutor(max_workers=max_concurrent_requests) as executor:
        print(f"[DEBUG] å¼€å§‹æäº¤ä»»åŠ¡...")
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_action = {}
        for action in candidate_actions:
            print(f"[DEBUG] æäº¤ä»»åŠ¡: {action}")
            future = executor.submit(_generate_and_evaluate_action, action, state_data)
            future_to_action[future] = action
        print(f"[DEBUG] æ‰€æœ‰ä»»åŠ¡æäº¤å®Œæˆï¼Œå…± {len(future_to_action)} ä¸ªä»»åŠ¡")

        # æ”¶é›†ç»“æœ
        results = []
        for future in as_completed(future_to_action):
            try:
                print(f"[DEBUG] ç­‰å¾…ä»»åŠ¡å®Œæˆ...")
                result = future.result()
                print(f"[DEBUG] ä»»åŠ¡å®Œæˆï¼Œç»“æœç±»å‹: {type(result)}")
                results.append(result)
            except Exception as e:
                action = future_to_action[future]
                print(f"[DEBUG] ä»»åŠ¡ {action} æ‰§è¡Œå¤±è´¥ï¼Œé”™è¯¯ç±»å‹: {type(e)}")
                print(f"[DEBUG] é”™è¯¯ä¿¡æ¯: {e}")
                import traceback
                print(f"[DEBUG] å®Œæ•´é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
                results.append(e)



    for i, result in enumerate(results):
        action = candidate_actions[i]
        if isinstance(result, Exception):
            new_monologue.append(f"  - [{action}] åœ¨å¹¶è¡Œæ‰§è¡Œä¸­æ•è·åˆ°è‡´å‘½é”™è¯¯: {result}")
        else:
            # å…¼å®¹è¿”å› 2 å…ƒç»„æˆ– 3 å…ƒç»„
            if isinstance(result, tuple) and len(result) == 3:
                evaluated_response, monologue_entry, usage_info = result
                try:
                    round_input_tokens += int(usage_info.get("input_tokens", 0) or 0)
                    round_output_tokens += int(usage_info.get("output_tokens", 0) or 0)
                    round_total_tokens += int(usage_info.get("total_tokens", 0) or 0)
                except Exception:
                    pass
            else:
                evaluated_response, monologue_entry = result
            if evaluated_response:
                evaluated_responses.append(evaluated_response)
            if monologue_entry:
                new_monologue.append(monologue_entry)

    verbose = state_data.get("verbose", False)
    if verbose:
        print(f"[DEBUG] ç”Ÿæˆè¯„ä¼°èŠ‚ç‚¹: è¯„ä¼°äº† {len(evaluated_responses)} ä¸ªå€™é€‰å›å¤")

    if not evaluated_responses:
        new_monologue.append("æ‰€æœ‰æ¨¡å—éƒ½æ‰§è¡Œå¤±è´¥äº†ï¼Œä½¿ç”¨ç´§æ€¥å…œåº•å›å¤")
        # ğŸ›¡ï¸ å¤šçº§å…œåº•æœºåˆ¶
        try:
            # å°è¯•äººå·¥è½¬æ¥ - æ­¤å¤„ä¸ºç´§æ€¥æƒ…å†µï¼Œç›´æ¥è¿”å›å›ºå®šå›å¤
            final_response = "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æœ‰ç‚¹å¿™\n æ™šç‚¹å†è”ç³»æ‚¨"
        except Exception as e:
            new_monologue.append(f"ç”Ÿæˆç´§æ€¥å›å¤æ—¶ä¹Ÿå¤±è´¥äº†: {e}")
            # æœ€ç»ˆå…œåº•ï¼šå›ºå®šå›å¤
            final_response = "ç¨ç­‰å“ˆ æœ‰ç‚¹å¿™"
        debug_info = DebugInfo(
            current_stage=state_data.get("current_stage"),
            emotional_state=state_data.get("emotional_state").model_dump() if state_data.get(
                "emotional_state") else None,
            internal_monologue=new_monologue,
        )

        return {
            "final_response": final_response,
            "last_message": final_response,
            "debug_info": debug_info,
            # å³ä½¿å…œåº•ï¼Œä¹Ÿè¿”å›ç´¯è®¡çš„å½“è½® token
            "round_token_used": int(round_total_tokens)
        }
    debug_info = DebugInfo(
        current_stage=state_data.get("current_stage"),
        emotional_state=state_data.get("emotional_state").model_dump() if state_data.get(
            "emotional_state") else None,
        internal_monologue=new_monologue,
    )

    return {
        "evaluated_responses": evaluated_responses,
        "debug_info": debug_info,
        # å¯¹è¯ä¸­æœ¬è½®æ‰€æœ‰å€™é€‰åŠ¨ä½œï¼ˆç”Ÿæˆ+è¯„ä¼°ï¼‰çš„ token æ€»å’Œ
        "round_token_used": int(round_total_tokens)
    }

@tool
def self_verification_node(state_data: dict):
    """
    ä»è¯„ä¼°è¿‡çš„å€™é€‰é¡¹ä¸­é€‰æ‹©æœ€ä½³å“åº”ã€‚
    """
    evaluated_responses = state_data.get("evaluated_responses", [])
    debug_info = state_data.get("debug_info", DebugInfo())
    internal_monologue = debug_info.internal_monologue if debug_info and debug_info.internal_monologue else []

    # ä¸å†éœ€è¦ä»è¿™é‡Œè·å–é‡‡æ ·å™¨ï¼Œå› ä¸ºè¯„åˆ†å·²åœ¨ generate_and_evaluate_node å®Œæˆ
    # sampler = ...

    # ğŸ”§ ä¼˜åŒ–é€‰æ‹©é€»è¾‘ï¼šé™ä½è´¨é‡é—¨æ§›ï¼Œç¡®ä¿æ€»æ˜¯æœ‰å›å¤

    # å…ˆå°è¯•0.3ä»¥ä¸Šçš„å›å¤
    high_quality_responses = [r for r in evaluated_responses if r.get('score', 0.0) > 0.3]

    # å¦‚æœæ²¡æœ‰0.3ä»¥ä¸Šçš„ï¼Œå°è¯•0.2ä»¥ä¸Šçš„
    if not high_quality_responses:
        high_quality_responses = [r for r in evaluated_responses if r.get('score', 0.0) > 0.2]

    # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œé€‰æ‹©æ‰€æœ‰å›å¤ä¸­å¾—åˆ†æœ€é«˜çš„
    if not high_quality_responses and evaluated_responses:
        high_quality_responses = sorted(evaluated_responses, key=lambda x: x.get('score', 0.0), reverse=True)

    # æç«¯æƒ…å†µï¼šæ²¡æœ‰ä»»ä½•å›å¤
    if not high_quality_responses:
        new_monologue = internal_monologue + ["è‡ªæˆ‘éªŒè¯å¤±è´¥ï¼šæ²¡æœ‰å¯ä¾›é€‰æ‹©çš„å€™é€‰å›å¤ï¼Œä½¿ç”¨ç´§æ€¥å›å¤"]
        fallback_response = "å—¯å—¯ï¼Œå¥½çš„"  # ç®€å•è‡ªç„¶çš„å…œåº•å›å¤
        debug_info = DebugInfo(
            current_stage=state_data.get("current_stage"),
            emotional_state=state_data.get("emotional_state").model_dump() if state_data.get(
                "emotional_state") else None,
            internal_monologue=new_monologue,
        )
        return {
            "last_message": fallback_response,
            "debug_info": debug_info
        }

    if len(high_quality_responses) == 1:
        final_response = high_quality_responses[0]['response']
        new_monologue = internal_monologue + [
            f"è‡ªæˆ‘éªŒè¯ï¼šåªæœ‰1ä¸ªé«˜è´¨é‡é€‰é¡¹ï¼Œç›´æ¥é€‰æ‹© '{high_quality_responses[0]['action']}'ã€‚"]
    else:
        # ç›´æ¥æŒ‰å¾—åˆ†æ’åºé€‰æ‹©æœ€é«˜åˆ†çš„å›å¤
        best_response = sorted(high_quality_responses, key=lambda x: x['score'], reverse=True)[0]
        final_response = best_response['response']
        new_monologue = internal_monologue + [
            f"è‡ªæˆ‘éªŒè¯ï¼šä» {len(high_quality_responses)} ä¸ªé€‰é¡¹ä¸­é€‰æ‹©å¾—åˆ†æœ€é«˜çš„å›å¤ (æ¨¡å—: {best_response['action']}, å¾—åˆ†: {best_response['score']:.2f})ã€‚"
        ]

    # --- å…³é”®æ”¹åŠ¨ï¼šå°†AIçš„æœ€ç»ˆå›å¤æ›´æ–°å›æ¶ˆæ¯å†å²ä¸­ ---
    # With `add_messages`, we just need to return the new message(s) in a list.
    # LangGraph will handle appending it to the state.
    new_monologue.append("å°†AIçš„æœ€æ–°å›å¤è¿”å›ï¼Œç”±LangGraphè‡ªåŠ¨æ›´æ–°å†å²ã€‚")



    verbose = state_data.get("verbose", False)


    if verbose:
        print(f"[DEBUG] æœ€ç»ˆå›å¤: {final_response}")

    # --- æ–°å¢ï¼šå¦‚æœ verbose æ¨¡å¼å¼€å¯ï¼Œåˆ™å‡†å¤‡è°ƒè¯•ä¿¡æ¯ ---
    debug_info = DebugInfo(
            current_stage=state_data.get("current_stage"),
            emotional_state=state_data.get("emotional_state").model_dump() if state_data.get("emotional_state") else None,
            internal_monologue=new_monologue,
        )
    return {
        "last_message": final_response,  # æ–°å¢ï¼šç”¨äºAPIè¾“å‡º
        "debug_info": debug_info,  # å°†è°ƒè¯•ä¿¡æ¯æ·»åŠ åˆ°è¿”å›å­—å…¸ä¸­
    }
