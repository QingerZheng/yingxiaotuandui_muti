"""
æ–‡æ¡£æ³¨å…¥çŸ¥è¯†åº“API
æ”¯æŒæ ¼å¼: PDFã€Wordã€æ–‡æœ¬ã€å›¾ç‰‡ã€PPTã€Excelç­‰

ä½¿ç”¨æ–¹å¼:
1. å¯åŠ¨æœåŠ¡: langgraph dev
2. è°ƒç”¨API: 
   POST /doc_ingestion_agent
   {
       "file_urls": ["file_path_or_url"]
   }
"""
import os
from typing import List
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, END
# ä¸ºäº†å…¼å®¹ Python 3.11 ç‰ˆæœ¬æ·»åŠ 
from typing_extensions import TypedDict

from rag.embedding import embedding_docs
from rag.utils.rag_utils import download_doc, load_and_chunk_document
from rag import milvus_client

class BatchGraphStateInput(TypedDict):
    """
    APIè¾“å…¥æ ¼å¼
    
    Example:
        {
            "file_urls": [
                "https://example.com/doc.pdf",
                "C:/Users/docs/file.docx",
                "/absolute/path/to/file.pdf",
                "relative/path/to/file.txt"
            ]
        }
    """
    file_urls: List[str]

class BatchGraphState(BatchGraphStateInput):
    """å¤„ç†çŠ¶æ€"""
    file_urls: List[str]
    processed_count: int
    failed_count: int
    error: str
    messages: List[str]

def check_document_exists(filename: str) -> bool:
    """æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²å­˜åœ¨"""
    try:
        # ä½¿ç”¨ç®€å•çš„å­—ç¬¦ä¸²åŒ¹é…ï¼Œé¿å…JSONæ ¼å¼é—®é¢˜
        results = milvus_client.query(
            collection_name="company_info_primary_key",
            filter=f"source like '%{filename}%'",
            output_fields=["source"],
            limit=1
        )
        return len(results) > 0
    except Exception as e:
        print(f"âš ï¸  æ£€æŸ¥æ–‡æ¡£å­˜åœ¨æ€§å¤±è´¥: {e}")
        return False

def delete_existing_document(filename: str) -> int:
    """åˆ é™¤å·²å­˜åœ¨çš„æ–‡æ¡£"""
    try:
        results = milvus_client.query(
            collection_name="company_info_primary_key",
            filter=f"source like '%{filename}%'",
            output_fields=["source"],
            limit=1000
        )
        
        delete_count = len(results)
        if delete_count > 0:
            milvus_client.delete(
                collection_name="company_info_primary_key",
                filter=f"source like '%{filename}%'"
            )
        return delete_count
    except Exception as e:
        print(f"âš ï¸  åˆ é™¤æ–‡æ¡£å¤±è´¥: {e}")
        return 0

def batch_ingest_docs_node(state: BatchGraphState):
    """
    å¤„ç†æ–‡æ¡£å¹¶æ³¨å…¥çŸ¥è¯†åº“
    
    æ”¯æŒ:
    1. ç½‘ç»œURL
    2. æœ¬åœ°ç»å¯¹è·¯å¾„
    3. æœ¬åœ°ç›¸å¯¹è·¯å¾„
    4. è‡ªåŠ¨æ–‡æ¡£å»é‡
    5. å¤šç§æ–‡æ¡£æ ¼å¼
    6. è‡ªåŠ¨æ¸…ç†ç½‘ç»œä¸‹è½½çš„ä¸´æ—¶æ–‡ä»¶
    """
    file_urls = state.get('file_urls', [])
    processed_count = 0
    failed_count = 0
    all_messages = []
    
    for i, file_url in enumerate(file_urls):
        local_path = None
        is_temp_file = False
        
        try:
            # 1. è·å–æœ¬åœ°æ–‡ä»¶è·¯å¾„
            # æ£€æŸ¥è¾“å…¥æ˜¯å¦ä¸ºJSONæ ¼å¼çš„å­—ç¬¦ä¸²
            if file_url.startswith('{') and file_url.endswith('}'):
                print(f"âš ï¸  æ£€æµ‹åˆ°JSONæ ¼å¼è¾“å…¥ï¼Œå°è¯•è§£æ: {file_url[:100]}...")
                import json
                try:
                    parsed_data = json.loads(file_url)
                    if 'file_urls' in parsed_data and len(parsed_data['file_urls']) > 0:
                        file_url = parsed_data['file_urls'][0]
                        print(f"âœ… ä»JSONä¸­æå–æ–‡ä»¶è·¯å¾„: {file_url}")
                except json.JSONDecodeError:
                    print(f"âŒ JSONè§£æå¤±è´¥: {file_url}")
                    continue
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºç½‘ç»œæ–‡ä»¶
            is_temp_file = file_url.startswith(('http://', 'https://'))
            local_path = download_doc(file_url) if is_temp_file else file_url
            filename = os.path.basename(local_path)
            
            # 2. æ£€æŸ¥å¹¶åˆ é™¤é‡å¤æ–‡æ¡£
            if check_document_exists(filename):
                deleted_count = delete_existing_document(filename)
                all_messages.append(f"æ–‡æ¡£å·²å­˜åœ¨ï¼Œå·²åˆ é™¤ {deleted_count} ä¸ªé‡å¤ç‰‡æ®µ: {filename}")
            
            # 3. åŠ è½½å¹¶åˆ†å—æ–‡æ¡£
            chunked_docs = load_and_chunk_document(local_path)
            
            # 4. ç”Ÿæˆå‘é‡
            doc_vectors = embedding_docs([doc.page_content for doc in chunked_docs])
            
            # 5. å‡†å¤‡æ•°æ®
            data = [
                {
                    "vector": doc_vectors[j],
                    "text": doc.page_content,
                    "source": doc.metadata.get("source", f"{filename}_chunk_{j}"),
                }
                for j, doc in enumerate(chunked_docs)
            ]

            # 6. æ’å…¥åˆ°Milvus
            milvus_client.insert(
                collection_name="company_info_primary_key",
                data=data,      
            )
            
            processed_count += 1
            all_messages.append(f"âœ… æˆåŠŸå¤„ç†æ–‡æ¡£: {filename} ({len(data)} ä¸ªç‰‡æ®µ)")
            
        except Exception as e:
            failed_count += 1
            error_msg = f"å¤„ç†å¤±è´¥ {file_url}: {str(e)}"
            all_messages.append(error_msg)
        
        finally:
            # 7. æ¸…ç†ç½‘ç»œä¸‹è½½çš„ä¸´æ—¶æ–‡ä»¶
            if is_temp_file and local_path and os.path.exists(local_path):
                try:
                    os.remove(local_path)
                    print(f"ğŸ—‘ï¸  å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {local_path}")
                except Exception as e:
                    print(f"âš ï¸  åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {local_path}, é”™è¯¯: {e}")
    
    return {
        "processed_count": processed_count,
        "failed_count": failed_count,
        "error": None if failed_count == 0 else "éƒ¨åˆ†æ–‡æ¡£å¤„ç†å¤±è´¥",
        "messages": all_messages
    }

# æ„å»ºå·¥ä½œæµ
workflow = StateGraph(BatchGraphState, input=BatchGraphStateInput)
workflow.add_node("batch_ingest_docs_node", batch_ingest_docs_node)
workflow.set_entry_point("batch_ingest_docs_node")
batch_doc_ingestion_workflow = workflow.compile()
