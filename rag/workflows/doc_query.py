"""
Defines the RAG (Retrieval-Augmented Generation) question-answering workflow.
This module is designed to be exposed as an API endpoint via `langgraph dev`.
"""
from typing import List, TypedDict
# ä¸ºäº†å…¼å®¹ Python 3.11 ç‰ˆæœ¬æ·»åŠ 
from typing_extensions import TypedDict
import os
import sys
from pathlib import Path
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = str(Path(__file__).parent.parent.parent)
if project_root not in sys.path:
    sys.path.append(project_root)

from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langgraph.graph import StateGraph, END

# Import core RAG components using absolute paths
from prompts.loader import load_prompt
from rag.milvus_wrapper import get_retriever
from rag.utils.rag_utils import load_chat_model
from rag.workflows.doc_listing import list_docs_node # å¯¼å…¥æ–‡æ¡£åˆ—è¡¨èŠ‚ç‚¹

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# 1. Define the state for the graph
class RAGState(TypedDict):
    """Represents the state of the RAG graph."""
    question: str
    answer: str

# 2. Load the prompt template and initialize components
try:
    # åŠ è½½æç¤ºè¯æ¨¡æ¿
    prompt_template_string = load_prompt("rag_prompt.txt", include_base_context=False)
    if not prompt_template_string:
        raise ValueError("RAGæç¤ºè¯æ¨¡æ¿åŠ è½½å¤±è´¥")
    RAG_PROMPT = PromptTemplate.from_template(prompt_template_string)
    print("âœ… RAGæç¤ºè¯æ¨¡æ¿åŠ è½½æˆåŠŸ")

    # åˆå§‹åŒ–æ£€ç´¢å™¨
    retriever = get_retriever()
    if not retriever:
        raise ValueError("æ£€ç´¢å™¨åˆå§‹åŒ–å¤±è´¥")
    print("âœ… æ£€ç´¢å™¨åˆå§‹åŒ–æˆåŠŸ")

    # åˆå§‹åŒ–è¯­è¨€æ¨¡å‹ï¼ˆé»˜è®¤ä½¿ç”¨ OpenRouter å¯ç”¨æ¨¡å‹ï¼‰
    model_name = os.getenv("NODE_MODEL", "x-ai/grok-3")
    llm = load_chat_model(model_name)
    if not llm:
        raise ValueError(f"è¯­è¨€æ¨¡å‹ {model_name} åˆå§‹åŒ–å¤±è´¥")
    print(f"âœ… è¯­è¨€æ¨¡å‹ {model_name} åˆå§‹åŒ–æˆåŠŸ")

except Exception as e:
    print(f"âŒ RAGåˆå§‹åŒ–å¤±è´¥: {str(e)}")
    raise

# 3. Define a helper function to format retrieved documents
def format_docs(docs: List[Document]) -> str:
    """Formats a list of retrieved documents into a single string."""
    return "\n\n".join(doc.page_content for doc in docs)

# å®šä¹‰ä¸€ä¸ªæ–‡ä»¶ç‰¹å®šçš„æ£€ç´¢å™¨ç±»
class FileSpecificRetriever:
    """A wrapper that filters retrieval results to a specific file."""
    
    def __init__(self, base_retriever, target_file):
        self.base_retriever = base_retriever
        self.target_file = target_file
    
    def __call__(self, query):
        """Make the class callable like a function."""
        return self.base_retriever._get_relevant_documents(
            query, 
            run_manager=None, 
            current_file=self.target_file
        )

# 4. Construct the core RAG chain as a reusable function
def build_rag_chain(retriever):
    """Builds a RAG chain with a given retriever."""
    return (
        {
            "context": retriever | RunnableLambda(format_docs),
            "question": RunnablePassthrough(),
        }
        | RAG_PROMPT
        | llm
        | StrOutputParser()
    )

# 5. Define the single node for our graph
def generate_answer_node(state: RAGState):
    """
    æ™ºèƒ½RAGé—®ç­”èŠ‚ç‚¹
    
    åŠŸèƒ½ï¼š
    1. ä»çŠ¶æ€ä¸­è·å–ç”¨æˆ·é—®é¢˜
    2. æ™ºèƒ½æ£€æµ‹é—®é¢˜ä¸­æ˜¯å¦åŒ…å«çŸ¥è¯†åº“ä¸­çš„æ–‡ä»¶å
    3. å¦‚æœæ£€æµ‹åˆ°æ–‡ä»¶åï¼Œåˆ›å»ºæ–‡ä»¶ç‰¹å®šçš„æ£€ç´¢å™¨è¿›è¡Œç²¾å‡†æœç´¢
    4. å¦‚æœæœªæ£€æµ‹åˆ°æ–‡ä»¶åï¼Œä½¿ç”¨å…¨å±€æ£€ç´¢å™¨è¿›è¡Œå¸¸è§„æœç´¢
    5. æ„å»ºå¹¶æ‰§è¡ŒRAGé“¾ï¼Œç”Ÿæˆç­”æ¡ˆå¹¶æ›´æ–°çŠ¶æ€
    """
    try:
        question = state["question"]
        print(f"ğŸ“ å¤„ç†é—®é¢˜: {question}")
        
        # é»˜è®¤ä½¿ç”¨å…¨å±€çš„retriever
        current_retriever = retriever
        
        # æ™ºèƒ½æ£€æµ‹ï¼šæ£€æŸ¥é—®é¢˜ä¸­æ˜¯å¦åŒ…å«çŸ¥è¯†åº“ä¸­çš„æ–‡ä»¶å
        print("ğŸ” æ­£åœ¨æ£€æŸ¥é—®é¢˜ä¸­æ˜¯å¦åŒ…å«æ–‡ä»¶å...")
        docs_info = list_docs_node({})
        
        # è·å–çŸ¥è¯†åº“ä¸­æ‰€æœ‰æ–‡ä»¶ååˆ—è¡¨
        if docs_info and docs_info.get("files"):
            knowledge_base_files = [item['filename'] for item in docs_info['files']]
            found_filename = None
            
            # éå†æ‰€æœ‰æ–‡ä»¶åï¼ŒæŸ¥æ‰¾é—®é¢˜ä¸­æ˜¯å¦åŒ…å«ä»»ä½•ä¸€ä¸ª
            for filename in knowledge_base_files:
                if filename in question:
                    found_filename = filename
                    break
            
            if found_filename:
                print(f"âœ… åœ¨é—®é¢˜ä¸­æ£€æµ‹åˆ°æ–‡ä»¶å: {found_filename}")
                print(f"ğŸ¯ å°†åœ¨æ–‡ä»¶ '{found_filename}' ä¸­è¿›è¡Œå®šå‘æœç´¢...")
                # åˆ›å»ºæ–‡ä»¶ç‰¹å®šçš„æ£€ç´¢å™¨
                file_retriever = FileSpecificRetriever(retriever, found_filename)
                current_retriever = RunnableLambda(file_retriever)
            else:
                print("â¡ï¸ æœªæ£€æµ‹åˆ°ç‰¹å®šæ–‡ä»¶åï¼Œå°†åœ¨æ•´ä¸ªçŸ¥è¯†åº“ä¸­æœç´¢ã€‚")
        else:
            print("âš ï¸ æ— æ³•è·å–çŸ¥è¯†åº“æ–‡ä»¶åˆ—è¡¨ï¼Œå°†åœ¨æ•´ä¸ªçŸ¥è¯†åº“ä¸­æœç´¢ã€‚")

        # ä½¿ç”¨é€‰å®šçš„retrieveræ„å»ºå¹¶è°ƒç”¨RAGé“¾
        dynamic_rag_chain = build_rag_chain(current_retriever)
        answer = dynamic_rag_chain.invoke(question)
        print("âœ… å›ç­”ç”ŸæˆæˆåŠŸ")
        
        return {"answer": answer}
    except Exception as e:
        print(f"âŒ å›ç­”ç”Ÿæˆå¤±è´¥: {str(e)}")
        return {"answer": f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°é”™è¯¯: {str(e)}"}

# 6. Build and compile the graph
workflow = StateGraph(RAGState)
workflow.add_node("generate_answer", generate_answer_node)
workflow.set_entry_point("generate_answer")
workflow.add_edge("generate_answer", END)

# 7. Compile the graph
rag_query_workflow = workflow.compile()
print("âœ… RAGå·¥ä½œæµç¼–è¯‘å®Œæˆ")

def rag_answer(query: str) -> str:
    """
    ç®€åŒ–çš„RAGé—®ç­”æ¥å£
    
    Args:
        query: ç”¨æˆ·çš„æŸ¥è¯¢é—®é¢˜
        
    Returns:
        str: åŸºäºçŸ¥è¯†åº“çš„å›ç­”
    """
    try:
        result = rag_query_workflow.invoke({
            "question": query,
            "answer": ""  # åˆå§‹åŒ–ä¸ºç©º
        })
        
        if not result or not result.get("answer"):
            raise ValueError("æœªèƒ½ç”Ÿæˆæœ‰æ•ˆå›ç­”")
            
        return result["answer"]
        
    except Exception as e:
        error_msg = f"RAGæŸ¥è¯¢å¤±è´¥: {str(e)}"
        print(f"âŒ {error_msg}")
        return error_msg
